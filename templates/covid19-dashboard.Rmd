---
title: "COVID-19 reddit Algo-Tracker" 
output:
    rmdformats::readthedown:
    html_document:
        toc: true
        theme: readthedown
        code_folding: hide
date: '`r format(Sys.Date(), "%B %d, %Y")`'
author: 
    - name: J. Nathan Matias
      url: "https://natematias.com"
      affiliation: Citizens and Technology Lab, Cornell
      affiliation_url: https://citizensandtech.org
    - name: Eric Pennington
      affiliation: Citizens and Technology Lab, Cornell
---
<style type="text/css">
/*#sidebar{background-color:#8d452f}
#sidebar h2{background-color:#ea5324}*/
h1,h2,h3,h4,h5,h6{color:#005073}
strong.headline{background:#ea532444}

#main a{color:#ea5324;
        background-image: linear-gradient(180deg,#ea5324,#ea5324);
        font-weight:normal;}

#sidebar h2{background-color:#005073}
#sidebar{background-color:#121d21}
#postamble{border-top: solid 10px #005073;}
.author .glyphicon-user{display: none !important}

</style>

```{r global_options, include=FALSE}
knitr::opts_chunk$set(echo=FALSE, warning=FALSE, message=FALSE)
```


````{r results=FALSE, echo=FALSE}
## load libraries
library(ggplot2)
library(DescTools)
library(knitr)
library(rjson)
library(lubridate)
#library(magick)

## use optipng and pngquant to optimize pngs
## http://zevross.com/blog/2017/06/19/tips-and-tricks-for-working-with-images-and-figures-in-r-markdown-documents/#use-dpi-to-change-the-resolution-of-images-and-figures
knit_hooks$set(optipng = hook_optipng)
knit_hooks$set(pngquant = hook_pngquant)

## Set visual style
catpalette   <- c("#333333", "#ea5324", "#005073", "#7D868C", "#BDBBBB", "#F2F2F2","#F6F2EB")
covidpalette   <- c("#666666", "#ea5324", "#005073", "#7D868C", "#BDBBBB", "#F2F2F2","#F6F2EB")

chartpalette <- c("#ea5324", "#005073", "#7D868C", "#333333", "#F2F2F2","#BDBBBB", "#F6F2EB")

cat.theme <-  theme_bw() +
              theme(plot.title = element_text(size=13, face="bold", color=catpalette[3]),
                    axis.title.x =element_text(size=10, hjust = -0.01, color = catpalette[1]),
                    axis.title.y =element_text(size=10, color = catpalette[1]),
                    panel.background = element_rect(fill=catpalette[6]))
options(repr.plot.width=6, repr.plot.height=3)

## set directories and other global settings
HOME.DIR = file.path(getSrcDirectory(function() {}), "..")
most.recent.folder.name <- as.character(max(as.numeric(SplitPath(Sys.glob(
        file.path(HOME.DIR, "data-archive", "reddit", "[0-9]*")))$filename)))
DATA.FOLDER = file.path(HOME.DIR, "data-archive", "reddit", most.recent.folder.name)
Sys.glob(file.path(DATA.FOLDER, "*"))

## Load CAT Lab logo
#cat.logo.png <- image_read(file.path(HOME.DIR, "assets", "CAT-square-logo.png"))

## Basic Configurations
snapshot.datetime <- parse_date_time(most.recent.folder.name, orders="ymdHMS")
min.recommended.freq = 3

######################
## Utility Methods
cat.post <- function(post, ranking){
    cat(paste("<li> <small>(", post['rank_position'], ")",
              "(ðŸ”º",post['ups'], ")",
              "</small> <strong class='headline'>", 
              #<a href='https://np.reddit.com", 
              #post['permalink'], "'>", 
              post['title'], 
              "</strong>",#"</a></strong>",
              "<ul><li><small><strong>r/",post['subreddit'], 
              "</strong>. ",
              "(ðŸ’¬",post['num_comments.x'], ") ",
              "Domain: ",post['domain'], ". ",
              "Highest Rank: ", 
              post[paste("max_", ranking, sep="")], ". ",
              "Time on ", ranking, ": ", 
              as.integer(as.numeric(post[paste("front_", ranking, "_seconds", sep="")])/60.),
              " minutes</small></li></ul></small></li>", sep=""))
}

##############################
## load posts and rankings
hot.ranks <- read.csv(file.path(DATA.FOLDER, 
                               paste(most.recent.folder.name,
                                     "_rank_timeseries_hot",
                                     ".csv", sep="")))
hot.ranks$snapshot.num <- sequence(rle(as.character(hot.ranks$id))$lengths)


top.ranks <- read.csv(file.path(DATA.FOLDER, 
                               paste(most.recent.folder.name,
                                     "_rank_timeseries_top",
                                     ".csv", sep="")))
top.ranks$snapshot.num <- sequence(rle(as.character(top.ranks$id))$lengths)



recent.posts  <- read.csv(file.path(DATA.FOLDER, 
                               paste(most.recent.folder.name,
                                     "_promoted_posts",
                                     ".csv", sep="")))

## Record rank time and age in minutes
top.ranks$rank.time <- as.POSIXct(top.ranks$rank_time, origin="1970-01-01")
top.ranks$age.minutes <- as.integer((top.ranks$rank.time - max(top.ranks$rank.time))/60)

hot.ranks$rank.time <- as.POSIXct(hot.ranks$rank_time, origin="1970-01-01")
hot.ranks$age.minutes <- as.integer((hot.ranks$rank.time - max(hot.ranks$rank.time))/60)


config.json <- fromJSON(file=file.path(DATA.FOLDER, 
                               paste(most.recent.folder.name,
                               "_algotracker-config",
                                     ".json", sep="")))

observation.period.days <- as.integer(round(max(as.POSIXct(top.ranks$rank_time)) - min(as.POSIXct(top.ranks$rank_time))))

#####################################
## Create ranking and post subsets
latest.hot.ranking <- max(hot.ranks$rank_id)
latest.top.ranking <- max(top.ranks$rank_id)

latest.hot.ranks <- subset(hot.ranks, rank_id == latest.hot.ranking & is.na(ups)!=TRUE)
latest.hot.rank.ids <- unique(latest.hot.ranks$id)
latest.top.ranks <- subset(top.ranks, rank_id == latest.top.ranking & is.na(ups)!=TRUE)
latest.top.rank.ids <- unique(latest.top.ranks$id)

latest.hot.posts <- merge(subset(recent.posts, in_latest_snapshot_hot == 1), latest.hot.ranks, by=c("id"))
latest.top.posts <- merge(subset(recent.posts, in_latest_snapshot_top == 1), latest.top.ranks, by=c("id"))

latest.hot.posts <- latest.hot.posts[order(-latest.hot.posts$rank_position),]
latest.top.posts <- latest.top.posts[order(-latest.top.posts$rank_position),]

latest.hot.rank.snapshots <- subset(hot.ranks, id %in% latest.hot.rank.ids)
latest.top.rank.snapshots <- subset(top.ranks, id %in% latest.top.rank.ids)

## earliest hot and top rank IDs
earliest.hot.ranking <- min(hot.ranks$rank_id)
earliest.top.ranking <- min(top.ranks$rank_id)

earliest.hot.rank.ids <- unique(subset(hot.ranks, rank_id == earliest.hot.ranking & 
                                       is.na(ups)!=TRUE)$id)

earliest.top.rank.ids <- unique(subset(top.ranks, rank_id == earliest.top.ranking & 
                                       is.na(ups)!=TRUE)$id)

````

<!-- Matomo -->
<script type="text/javascript">
  if (window.location.hostname == "covid-algotracker.citizensandtech.org") {
    var _paq = window._paq || [];
    /* tracker methods like "setCustomDimension" should be called before "trackPageView" */
    _paq.push(['trackPageView']);
    _paq.push(['enableLinkTracking']);
    (function() {
      var u="https://citizensandtech.matomo.cloud/";
      _paq.push(['setTrackerUrl', u+'matomo.php']);
      _paq.push(['setSiteId', '1']);
      var d=document, g=d.createElement('script'), s=d.getElementsByTagName('script')[0];
      g.type='text/javascript'; g.async=true; g.defer=true; g.src='//cdn.matomo.cloud/citizensandtech.matomo.cloud/matomo.js'; s.parentNode.insertBefore(g,s);
    })();
  }
</script>
<!-- End Matomo Code -->

**How do reddit's popularity algorithms promote information about COVID-19?**


<!-- CAT Lab Logo (float right) -->
<img src="../assets/CAT-square-logo.png" style="width:30%; margin:10px; float:right;"/> 
Since 2016, the [Citizens and Technology Lab](https://citizensandtech.org/about-cat-lab/) (CAT Lab) has taken ongoing snapshots of reddit's algorithms every 2-3 minutes. We created this dashboard to inform the design of our collaborations with reddit communities on COVID-19 public health research. We have made the dashboard, data, and code public for researchers and practitioners who study the role of algorithms in society.

During a pandemic, people need evolving information to guide our health decisions and what we share with others. These needs continue across the months-long [pandemic cycle of prevention, resilience, and recovery](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4504362/). 

Reddit is already a major hub of COVID-19 information for hundreds of millions worldwide. News and norms spread rapidly across the site, promoted by people and algorithms. Reddit memes, jokes, and ideas are also shared widely on other platforms. While the reddit ecosystem could be a powerful force for good, public health experts have also argued that [cascades of human and algorithm sharing are the biggest pandemic risk](https://www.nature.com/articles/d41586-018-07034-4). And if you wonder whether this social news site matters, reddit reportedly has [more active users than Twitter](https://thenextweb.com/contributors/2018/04/19/reddit-now-active-users-twitter-engaging-porn/).

One reason to monitor algorithms is that information interventions can have unexpected side-effects on ranking algorithms. In a large-scale experiment with r/worldnews on reddit, we found that [encouraging fact-checking influenced what reddit's popularity algorithms promoted by influencing human behavior](https://osf.io/m98b6/).

## How we collect data
<img src="../assets/algotracker-logo.png" style="margin:10px;width:30%; float:right;">
This dashboard is generated every six hours using the latest data within CAT Lab's research software. The output of this dashboard is based on the last `r observation.period.days` days (we are limiting how often we update the dashboard for ethics reasons listed below). It uses these sources:

* snapshots every 2 minutes of two key reddit algorithms:
    * reddit's HOT algorithm, the default popularity ranking on the site
    * reddit's TOP algorithm
* data about posts, currently queried from Jason Baumgartner's [PushShift service](https://pushshift.io/)

With every report, this software also publishes research-quality datasets to [covid-algotracker on github](https://github.com/natematias/covid-algotracker/tree/master/data-archive/reddit), along with information about the settings, keywords, and software behind the data.

This report is based on 3 days of data up to the following snapshots:

* Last HOT snapshot: `r as.character(latest.hot.ranks$rank_time[[1]])`
* Last TOP snapshot: `r as.character(latest.top.ranks$rank_time[[1]])`

## How we identify COVID-19 posts
This report identified a post as related to COVID-19 if the lower-case title or the text of the submission matched any of the following English language terms ([code here](https://github.com/natematias/covid-algotracker/blob/master/scripts/fetch-reddit-frontpage.py)). Terms will be updated as the pandemic evolves.

````{r echo = FALSE, results=TRUE}
config.json$terms
````

# How algorithms promote reddit posts over time
Reddit's public rankings are a classic recommender system. Using information from upvotes, downvotes, post age, and maybe other data, reddit's algorithms create a ranked list suggestions to readers. According to Christian Sandvig, feeds like this [outsource human knowledge and attention to machines](https://citizensandtech.org/2020/03/the-feed-sandvig-2020/). Hybrid human-machine curation can be [valuable and difficult during crises](https://dl.acm.org/doi/pdf/10.1145/2998181.2998299), as Alex Leavitt has described in extensive research on reddit.

This dashboard reviews posts that appeared anywhere in the top 100 recommendations for TOP and HOT. On average over the last `r observation.period.days` days, COVID-19 articles that appeared anywhere in the rankings stayed on HOT for `r sprintf("%.01f", mean(subset(recent.posts, (id %in% latest.hot.rank.ids)!=TRUE & (id %in% earliest.hot.rank.ids)!=TRUE & front_hot_seconds!=0 & covid_19==1)$front_hot_seconds)/60./60.)` hours and on TOP for `r sprintf("%.01f", mean(subset(recent.posts, (id %in% latest.top.rank.ids)!=TRUE & (id %in% earliest.top.rank.ids)!=TRUE & front_top_seconds!=0 & covid_19==1)$front_top_seconds )/60./60.)` hours.

HOT and TOP aren't the only algorithms that influence what people see. For several years now, reddit has provided a personalized newsfeed algorithm to logged in users, based on the communities that people subscribe to. The company has also developed BEST and POPULAR algorithms, which can add if there's interest.

Here's what that trajectory looks like for one randomly-sampled post that rose and fell from prominence. In the chart, a rank position of 0 means that the article is at the very top. People typically have to scroll down to see more than 3-4 posts. A rank position of -99 means that you would have to scroll past 99 other posts to see that one. In this example, notice that the HOT rankings are more volatile than TOP:

````{r echo=FALSE, results=FALSE, fig.width=8, fig.height=3, dpi=96, pngquant = '--quality 80-90'}

illustration.post.id <- as.character(sample(
    subset(recent.posts, (id %in% latest.hot.rank.ids)!=TRUE & 
                         (id %in% latest.top.rank.ids)!=TRUE & 
                         (id %in% earliest.top.rank.ids)!=TRUE & 
                         (id %in% earliest.top.rank.ids)!=TRUE & 
                         front_hot_seconds > 0 &
                         front_top_seconds > 0 & 
                         max_top > -50 & 
                         covid_19==1)$id, 1))
illustration.post.title <- paste(substr(as.character(recent.posts[recent.posts$id==illustration.post.id,][['title']]), 0,100), "...", sep="")
illustration.post.title


min.hot.xlim = floor(min(subset(hot.ranks, id==illustration.post.id & is.na(rank_position)!=T)$age.minutes)/60)*60
max.hot.xlim = floor(max(subset(hot.ranks, id==illustration.post.id & is.na(rank_position)!=T)$age.minutes)/60)*60
min.top.xlim = floor(min(subset(top.ranks, id==illustration.post.id & is.na(rank_position)!=T)$age.minutes)/60)*60
max.top.xlim = floor(max(subset(top.ranks, id==illustration.post.id & is.na(rank_position)!=T)$age.minutes)/60)*60

max.both.xlim = max(c(max.hot.xlim, max.top.xlim))
min.both.xlim = min(c(min.hot.xlim, min.top.xlim))

## Illustrate the trajectory for HOT

ggplot(subset(hot.ranks, id==illustration.post.id), aes(age.minutes, rank_position)) +
    geom_line(color=covidpalette[2]) +
    cat.theme +
    scale_colour_manual(values = covidpalette, name="COVID-19\nrelated") +
    ylab("Rank position")  +
    scale_x_continuous(breaks=seq(min.both.xlim, max.both.xlim, 60), limits=c(min.both.xlim, max.both.xlim)) + 
    ylim(-100,0) +
    ggtitle(paste("Example trajectory of COVID-19 Post across reddit's HOT rankings:\n",
                  '"', illustration.post.title, '"', sep="")) +
    xlab(paste("Algorithm rank position over time for a reddit post (covering ",
               abs((max.both.xlim - min.both.xlim)/60), 
               " hours). Top: 0.  Bottom: -99 \n",
               "Interruptions can be caused by community removal. ",
               "Last snapshot: ", snapshot.datetime, ".\n",
               "Citizens & Technology Lab, Cornell University. CC BY-ND 4.0.\n",
               "Report and data: covid-algotracker.citizensandtech.org", sep=""))

### Illustrate the trajectory for TOP
ggplot(subset(top.ranks, id==illustration.post.id), aes(age.minutes, rank_position)) +
    geom_line(color=covidpalette[2]) +
    cat.theme +
    scale_colour_manual(values = covidpalette, name="COVID-19\nrelated") +
    scale_x_continuous(breaks=seq(min.both.xlim, max.both.xlim, 60), limits=c(min.both.xlim, max.both.xlim)) + 
    ylim(-100,0) +
    ylab("Rank position")  +
    ggtitle(paste("Example Trajectory of COVID-19 Post across reddit's TOP rankings:\n",
                  '"', illustration.post.title, '"', sep="")) +
    xlab(paste("Algorithm rank position over time for a reddit post (covering ",
               abs((max.both.xlim - min.both.xlim)/60), 
               " hours). Top: 0.  Bottom: -99\n",
               "Interruptions can be caused by community removal. ",
               "Last snapshot: ", snapshot.datetime, ".\n",
               "Citizens & Technology Lab, Cornell University. CC BY-ND 4.0.\n",
               "Report and data: covid-algotracker.citizensandtech.org", sep=""))



````

# Currently-promoted COVID-19 posts on reddit

## Posts currently promoted by the HOT algorithm

In the latest snapshot, `r sprintf("%.01f", (sum(latest.hot.posts$covid_19.x) / nrow(latest.hot.posts))*100)`% of posts appearing in the HOT algorithm on reddit are COVID-19 related.



<ul>
<li><strong>(rank 0 to -99) (upvotes) (title)</strong></li>
````{r echo = FALSE, results='asis'}
x <- apply(subset(latest.hot.posts, covid_19.x==1), 1, FUN=cat.post, ranking="hot")
````
</ul>

````{r echo = FALSE, results=FALSE, fig.width=8, fig.height=3, dpi=96, pngquant = '--quality 80-90'}

min.hot.xlim = floor(min(subset(latest.hot.rank.snapshots, is.na(rank_position)!=T)$age.minutes)/60)*60

ggplot(latest.hot.rank.snapshots, aes(age.minutes, rank_position, group=id, col=factor(covid_19))) +
    geom_line() + 
    cat.theme +
    scale_colour_manual(values = covidpalette, name="COVID-19\nrelated") +
    scale_x_continuous(breaks=seq(min.hot.xlim, 0, 60), limits=c(min.hot.xlim, 0)) + 
    ylab("Rank position")  +
    ggtitle(paste("COVID-19 Posts Promoted by reddit's HOT Algorithm, ", snapshot.datetime, sep="")) +
    xlab(paste("Minutes before the most recent snapshot, at ", snapshot.datetime, ".\n\n",
               "Rank position over time of ",length(latest.hot.rank.ids)," posts currently appearing on reddit's HOT algorithm.  Top: 0.  Bottom: -99. \n",
               "Citizens & Technology Lab, Cornell University. CC BY-ND 4.0.\n",
               "Report and data: covid-algotracker.citizensandtech.org", sep=""))

````

## Posts currently promoted by the TOP algorithm


In the latest snapshot, `r sprintf("%.01f", (sum(latest.top.posts$covid_19.x) / nrow(latest.top.posts))*100)`% of posts appearing in the TOP algorithm on reddit are COVID-19 related.

<ul>
<li><strong>(rank 0 to -99) (upvotes) (title)</strong></li>
````{r echo = FALSE, results='asis'}
x <- apply(subset(latest.top.posts, covid_19.x==1), 1, FUN=cat.post, ranking="top")
````
</ul>


````{r echo=FALSE, results=FALSE, fig.width=8, fig.height=3, dpi=96, pngquant = '--quality 80-90'} 
min.top.xlim = floor(min(subset(latest.top.rank.snapshots, is.na(rank_position)!=T)$age.minutes)/60)*60

ggplot(latest.top.rank.snapshots, aes(age.minutes, rank_position, group=id, col=factor(covid_19))) +
    geom_line() + 
    cat.theme +
    scale_colour_manual(values = covidpalette, name="COVID-19\nrelated") +
    scale_x_continuous(breaks=seq(min.top.xlim, 0, 60), limits=c(min.top.xlim, 0)) + 
    ylab("Rank position")  +
    ggtitle(paste("COVID-19 Posts Promoted by reddit's TOP Algorithm, ", snapshot.datetime, sep="")) +
    xlab(paste("Minutes before the most recent snapshot, at ", snapshot.datetime, ".\n\n",
               "Rank position over time of ",length(latest.top.rank.ids),
               " posts currently appearing on reddit's TOP algorithm.  Top: 0.  Bottom: -99. \n",
               "Citizens & Technology Lab, Cornell University. CC BY-ND 4.0.\n",
               "Report and data: covid-algotracker.citizensandtech.org", sep=""))
````

# Top communities promoted by reddit's algorithms

Reddit's algorithms recommended these subreddits at least `r min.recommended.freq` times over the last `r observation.period.days` days.

```{r echo=FALSE, results=TRUE}

recent.subs <- as.data.frame(table(as.character(subset(recent.posts, covid_19==1)$subreddit)))
recommended.subs <- subset(recent.subs[order(-recent.subs$Freq),], Freq>=min.recommended.freq)
row.names(recommended.subs) <- recommended.subs$Var1
names(recommended.subs) <- c("subreddit", "frequency")
kable(recommended.subs[c("frequency")],
    caption=paste("Top <strong>subreddit communities</strong> with COVID-19 content promoted by reddit's algorithms in the past ",
                  observation.period.days, " days.",sep=""))

````


# Top domains promoted by reddit's algorithms
These domains were recommended at least `r min.recommended.freq` times by HOT or TOP over the last `r observation.period.days` days.
```{r echo=FALSE, results=TRUE}
recent.domains <- as.data.frame(table(as.character(subset(recent.posts, covid_19==1)$domain)))
recommended.domains <- subset(recent.domains[order(-recent.domains$Freq),], Freq>=min.recommended.freq)
row.names(recommended.domains) <- recommended.domains$Var1
names(recommended.domains) <- c("domain", "frequency")
kable(recommended.domains[c("frequency")],
    caption=paste("Top <strong>web domains</strong> with COVID-19 content promoted by reddit's algorithms in the past ",
                  observation.period.days, " days.",sep=""))

````

# Ethics
This dashboard summarizes and archives information that is currently public on reddit's front page. To reduce risks to communities, this dashboard does not link directly to discussions. All author information is removed from the archival datasets.

You can read more about CAT Lab's ethics values and processes in our post on [strategies for ethical, accountable online behavior research](https://medium.com/@natematias/3-strategies-for-accountable-ethical-online-behavioral-research-f6ae134458fc).

# Don't use this dashboard for interventions (yet)
At CAT Lab, we believe that [powerful digital interventions ought to be tested](https://medium.com/mit-media-lab/the-obligation-to-experiment-83092256c3e9) to discover if they help rather than harm, and to enable public accountability. Interventions that form feedback loops with algorithms can have unpredictable side-effects, and we advise against proceeding without evaluation. To add extra friction to interventions, we are only updating the dashboard every six hours. The research data will still provide fine-grained information for people working to study and model reddit's algorithm behavior by the minute.

We are currently talking to funders about a project we developed with reddit communities and public health experts to test public health information interventions with people and algorithms. If you have ideas for funding opportunities, please contact J. Nathan Matias at nathan.matias@cornell.edu

# Questions and bugs

If you find a bug or have a question, please [post it to the issues page](https://github.com/natematias/covid-algotracker/issues) for this project on github. Thanks!

# About CAT Lab
The [Citizens and Technology Lab](https://citizensandtech.org) at Cornell University works with communities to study the social impacts of digital technologies and discover effective ideas for change. CAT Lab is led by [Dr. J. Nathan Matias](https://natematias.com), an assistant professor in the department of Communication. 

Working alongside communities and volunteers, we discover practical knowledge that also contributes to science, holds companies accountable, and is guided by the people most affected. Communities bring their problems, deep knowledge, and desire for change. We bring expertise in scientific research and a software platform for coordinating citizen behavioral science.

Over a dozen communities with tens of millions of people have worked with CAT Lab since 2016 on reddit, Wikipedia, and Twitter. We have tested practical ways to prevent harassment, fight misinformation, broaden inclusion, manage civic discourse, and protect freedom of expression. Our discoveries have directly influenced community practice, corporate policies, and government discussions worldwide. These [industry-independent](https://citizensandtech.org/2020/01/industry-independent-research/) findings are regularly published by the world's top scientific journals in the social sciences and computer science. 

# References and further reading

* Leavitt, A., & Robinson, J. J. (2017). [Upvote My News: The Practices of Peer Information Aggregation for Breaking News on reddit.com](https://dl.acm.org/doi/abs/10.1145/3134700). Proceedings of the ACM on Human-Computer Interaction, 1(CSCW), 1-18.
* Leavitt, A., & Robinson, J. J. (2017, February). [The role of information visibility in network gatekeeping: Information aggregation on reddit during crisis events](https://dl.acm.org/doi/pdf/10.1145/2998181.2998299). In Proceedings of the 2017 ACM conference on computer supported cooperative work and social computing (pp. 1246-1261).
* Larson, H. J. (2018). [The biggest pandemic risk? Viral misinformation](https://www.nature.com/articles/d41586-018-07034-4). Nature, 562(7726), 309-310.
* Matias, J. N. (2019). [Preventing harassment and increasing group participation through social norms in 2,190 online science discussions](https://www.pnas.org/content/early/2019/04/23/1813486116). Proceedings of the National Academy of Sciences, 116(20), 9785-9789.
* Matias, J. N. (pre-print, 2018). [Nudging Algorithms by Influencing Human Behavior. Effects of Encouraging Fact-Checking on News Algorithms](https://osf.io/m98b6/). https://osf.io/m98b6/
* Matias, J. N., & Mou, M. (2018, April). [CivilServant: Community-led experiments in platform governance](https://natematias.com/media/Community_Led_Experiments-CHI_2018.pdf). In Proceedings of the 2018 CHI conference on human factors in computing systems (pp. 1-13).
* Vaughan, E., & Tinker, T. (2009). [Effective health risk communication about pandemic influenza for vulnerable populations](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4504362/). American Journal of Public Health, 99(S2), S324-S332.

# Copyright
The Citizens and Technology Lab makes this report and associated code available under a [Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International Public License.](https://github.com/natematias/covid-algotracker/blob/master/LICENSE)

<!-- Social Media Preview Image generation and metadata -->

````{r echo=FALSE, results=FALSE}

## Facebook link thumbnail image sizes: 1200 x 630

p <- ggplot(latest.top.rank.snapshots, aes(age.minutes, rank_position, group=id, col=factor(covid_19))) +
    geom_line(size=1.3) + 
    cat.theme +
    scale_x_continuous(breaks=seq(min.top.xlim, 0, 60), limits=c(min.top.xlim, 0)) +
    theme(legend.position = c(0.002, 0.99), 
          legend.justification = c(0, 1),
          legend.text = element_text(size=24, face="bold"),
          legend.title=element_text(size=24, face="bold")) +
    theme(axis.title=element_blank(),
          axis.text=element_blank(),
          axis.ticks=element_blank()) +
    scale_colour_manual(values = covidpalette, name="COVID-19\nrelated") +
    xlab("") + ylab("") +
    ggsave(file.path(HOME.DIR, "assets", "preview-image.png"), width=12, height=6.3, dpi=100)

````

